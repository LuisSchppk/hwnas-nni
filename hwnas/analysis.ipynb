{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8e87b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import ast\n",
    "from search_space.search_space import VGG8ModelSpaceVGG8\n",
    "from search_space.util import combine_model_dict\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import torch\n",
    "from hwnas.hw_performance_estimation.hw_performance_estimation import get_hardware_metrics\n",
    "from evaluator.evaluator import test_epoch, train_epoch, objective\n",
    "import util\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d7b26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../output_short_training/results_evo.csv\", header=None)\n",
    "df.columns = [\"context\", \"accuracy\", \"latency\", \"energy\", \"area\"]\n",
    "\n",
    "df[\"metric\"] = df.iloc[:,1:5].apply(lambda row: objective(*row, w_acc=100, w_lat=1, w_en=1), axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8be3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the raw data\n",
    "ax.plot(df.index, df.metric)\n",
    "# ax.plot(df.index, df.energy, label='energy')\n",
    "\n",
    "# Add trend line\n",
    "df_with_index = df.reset_index()\n",
    "model = sm.formula.ols(formula='metric ~ index', data=df_with_index)\n",
    "res = model.fit()\n",
    "ax.plot(df.index, res.fittedvalues, '--', label='Trend')\n",
    "\n",
    "# After plotting your data and trendline:\n",
    "textstr = '\\n'.join((\n",
    "    r'$Slope=%.4f$' % (res.params['index'], ),\n",
    "    r'$p=%.4f$' % (res.pvalues['index'], ),\n",
    "    r'$R^2=%.3f$' % (res.rsquared, )))\n",
    "\n",
    "# Prepare annotation text\n",
    "slope = res.params['index']\n",
    "pval = res.pvalues['index']\n",
    "r2 = res.rsquared\n",
    "\n",
    "textstr = f\"Trend: {slope:.3f} (p = {pval:.3f}, RÂ² = {r2:.3f})\"\n",
    "\n",
    "# Add text box in the top-left corner of the plot\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=0.8)\n",
    "ax.text(0.02, 0.07, textstr, transform=ax.transAxes, fontsize=10,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "\n",
    "\n",
    "# ax.legend()\n",
    "ax.set_xlabel('Trial')\n",
    "ax.set_ylabel('Score')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"../../Paper/figs/metric_trend.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b250a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4389f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "top_k = df.sort_values('metric', ascending=False).head(k)\n",
    "context_list = top_k['context'].tolist()\n",
    "state_dicts = [{k: v for k, v in ast.literal_eval(d).items() if k != '__arch__'} for d in context_list]\n",
    "state_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e7d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_options, param_distr = combine_model_dict(state_dicts)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "\n",
    "label_rename_map = {\n",
    "    'pool_choice': 'Pooling Choice {0,1}',\n",
    "    'conv6_choice': 'Mutable Layer 6 Choice {0,1}',\n",
    "    'conv4_choice': 'Mutable Layer 4 Choice {0,1}',\n",
    "    'conv3_choice': 'Mutable Layer 3 Choice {0,1}',\n",
    "    'conv2_choice': 'Mutable Layer 2 Choice {0,1}',\n",
    "    'kernel_size_conv4': 'Conv4 Kernel Size {3,5}',\n",
    "    'kernel_size_conv3': 'Conv3 Kernel Size {3,5}',\n",
    "    'kernel_size_conv2': 'Conv2 Kernel Size {3,5,7}',\n",
    "    'kernel_size_conv1': 'Conv1 Kernel Size {3,5,7}' ,\n",
    "    'channel_multiplier': 'Growth Factor {1, 1.5, 2}',\n",
    "    'out_size_conv1': 'Channel Size {16, 32, 64}'\n",
    "}\n",
    "order = list(label_rename_map.keys())\n",
    "\n",
    "y_labels = list(param_options.keys())\n",
    "y_pos = np.arange(len(y_labels))\n",
    "\n",
    "params_ord = OrderedDict()\n",
    "\n",
    "for key in order:\n",
    "    params_ord.update({label_rename_map.get(key) : param_options.get(key)})\n",
    "max_choices = max(len(v) for v in params_ord.values())\n",
    "colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#592A37']\n",
    "bottom = np.zeros(len(y_labels))\n",
    "\n",
    "\n",
    "for choice_idx in range(max_choices):\n",
    "    heights = []\n",
    "    labels = []\n",
    "    for key in order:\n",
    "        param_values = params_ord[label_rename_map[key]]\n",
    "        param_distribution = param_distr[key]\n",
    "        if choice_idx < len(param_values):\n",
    "            heights.append(param_distribution[choice_idx])\n",
    "            labels.append(f\"{param_values[choice_idx]}\")\n",
    "        else:\n",
    "            heights.append(0)\n",
    "            labels.append(\"\")\n",
    "    bars = ax.barh(y_pos, heights, left=bottom, label=f'Value: {labels[0] if labels.count(labels[0]) == len(labels) else f\"Option {choice_idx}\"}', color=colors[choice_idx % len(colors)],)\n",
    "\n",
    "    for i, (bar, height, label) in enumerate(zip(bars, heights, labels)):\n",
    "        # if height > 0.05:  # Only show labels for reasonably sized segments\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, \n",
    "                    bar.get_y() + bar.get_height()/2, \n",
    "                    label, \n",
    "                    ha='center', va='center', \n",
    "                    fontsize=8,\n",
    "                    color='white')\n",
    "\n",
    "    bottom += heights\n",
    "\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(list(params_ord.keys()))\n",
    "ax.set_xlabel('Distribution')\n",
    "ax.set_title(f'Parameter Distributions of Top {k}')\n",
    "# ax.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"../../Paper/figs/distribution_top{k}.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170d61a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_space = VGG8ModelSpaceVGG8()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eabbdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_top_model(\n",
    "    model,\n",
    "    max_epochs=100,\n",
    "    batch_size=128,\n",
    "    num_workers=0,  # for Windows\n",
    "    patience=15,\n",
    "    min_delta=1,\n",
    "    device=\"cuda\",\n",
    "    lr=0.01\n",
    "):\n",
    "    print(\"Architecture \\n\", model)\n",
    "    counter=0\n",
    "    best_metric=-np.inf\n",
    "    util.replace_conv_bias_with_bn(module=model, device=device)\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                            (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                            (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    full_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "    train_dataset, val_dataset = random_split(full_train, [45000, 5000])\n",
    "    train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    accuracy = 0\n",
    "    for epoch in range(max_epochs):\n",
    "        print(\"Epoch\", epoch)\n",
    "        train_epoch(model, device, train_loader, optimizer)\n",
    "        accuracy = test_epoch(model, device, val_loader)\n",
    "        val_metric = accuracy\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "        if val_metric > best_metric + min_delta:\n",
    "            best_metric = val_metric\n",
    "            counter = 0\n",
    "            print(\"Reset Patience\")\n",
    "        else:\n",
    "            counter += 1\n",
    "            \n",
    "    test_accuracy = test_epoch(model, device, test_loader)\n",
    "    print(f\"Final Test Accuracy Original: {test_accuracy:.2f}\")\n",
    "    print(\"Training done.\")\n",
    "    acc, lat, eng, area = get_hardware_metrics(model, train_loader=train_loader, test_loader=test_loader, val_loader=val_loader, num_classes=10, sim_config=\"../MNSIM-2.0/SimConfig.ini\")\n",
    "    final_metric = objective(acc, lat, eng, area)\n",
    "    \n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Latency: {lat:.4f}\")\n",
    "    print(f\"Energy: {eng:.4f}\")\n",
    "    print(f\"Area: {area:.4f}\")\n",
    "    print(f\"Final Metric: {final_metric:.4f}\")\n",
    "    return final_metric, acc, lat, eng, area, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6194bd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_space.freeze(state_dicts[0])\n",
    "final_metric, acc, lat, eng, area, orig_accuracy = evaluate_top_model(model)\n",
    "    \n",
    "result_0 = {\n",
    "    \"index\": i,\n",
    "    \"final_metric\": final_metric,\n",
    "    \"accuracy\": acc,\n",
    "    \"latency\": lat,\n",
    "    \"energy\": eng,\n",
    "    \"area\": area,\n",
    "    \"orig_accuracy\" : orig_accuracy\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00afabd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_space.freeze(state_dicts[1])\n",
    "final_metric, acc, lat, eng, area, orig_accuracy = evaluate_top_model(model)\n",
    "    \n",
    "result_1 = {\n",
    "    \"index\": i,\n",
    "    \"final_metric\": final_metric,\n",
    "    \"accuracy\": acc,\n",
    "    \"latency\": lat,\n",
    "    \"energy\": eng,\n",
    "    \"area\": area,\n",
    "    \"orig_accuracy\" : orig_accuracy\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b2f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_space.freeze(state_dicts[2])\n",
    "final_metric, acc, lat, eng, area, orig_accuracy = evaluate_top_model(model)\n",
    "    \n",
    "result_2 = {\n",
    "    \"index\": i,\n",
    "    \"final_metric\": final_metric,\n",
    "    \"accuracy\": acc,\n",
    "    \"latency\": lat,\n",
    "    \"energy\": eng,\n",
    "    \"area\": area,\n",
    "    \"orig_accuracy\" : orig_accuracy\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d294b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(0, 301, 50):\n",
    "    model = model_space.freeze(state_dicts[i])\n",
    "    final_metric, acc, lat, eng, area, orig_accuracy = evaluate_top_model(model)\n",
    "    results.append({\n",
    "        'index': i,\n",
    "        'final_metric': final_metric,\n",
    "        'accuracy': acc,\n",
    "        'latency': lat,\n",
    "        'energy': eng,\n",
    "        'area': area,\n",
    "        \"orig_accuracy\" : orig_accuracy\n",
    "    })\n",
    "df_eval = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8af734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.read_csv(\"../../df_eval.csv\", index_col=0)\n",
    "df_eval.rename(columns={\"index\": \"trial\"}, inplace=True)\n",
    "df_eval[\"accuracy\"] *= 100\n",
    "df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c81402",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(df_eval[\"trial\"], df_eval[\"accuracy\"], marker='o', label=\"CIM accuracy\")\n",
    "plt.plot(df_eval[\"trial\"], df_eval[\"orig_accuracy\"], marker='s', label=\"accuracy\")\n",
    "plt.xlabel(\"Trial\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"CIM Accuracy vs. Accuracy\")\n",
    "plt.legend()\n",
    "# plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29eba1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_row_pretrained = {\n",
    "    \"trial\": \"400\",\n",
    "    \"final_metric\" : 78.33013044688072,\n",
    "    \"accuracy\":  91.69034090909091,\n",
    "    \"latency\" : 5151851.44571189,\n",
    "    \"energy\" : 4448843.521209403,\n",
    "    \"area\" : 1390330993.5429137,\n",
    "    \"orig_accuracy\": 93.00426136363636\n",
    "}\n",
    "\n",
    "control_row_trained = {\n",
    "    \"trial\": \"350\",\n",
    "    \"final_metric\" : 18.102857719607986,\n",
    "    \"accuracy\" : 31.46306818181818,\n",
    "    \"latency\" : 5151851.44571189,\n",
    "    \"energy\" : 4448843.521209403,\n",
    "    \"area\" : 1390330993.5429,\n",
    "    \"orig_accuracy\" : 54.04829545454546\n",
    "}\n",
    "\n",
    "# Append to DataFrame\n",
    "# df_eval = pd.concat([df_eval, pd.DataFrame([control_row_trained])], ignore_index=True)\n",
    "# df_eval = pd.concat([df_eval, pd.DataFrame([control_row_pretrained])], ignore_index=True)\n",
    "# df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a742fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "31.46306818181818 - math.log(5151851.44571189, 10) - math.log(4448843.521209403, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb05f2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(df_eval[\"trial\"], df_eval[\"accuracy\"], marker='o', label=\"CIM Accuracy\")\n",
    "plt.plot(df_eval[\"trial\"], df_eval[\"orig_accuracy\"], marker='s', label=\"Accuracy\")\n",
    "plt.xlabel(\"Trial\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"CIM Accuracy vs.Accuracy\")\n",
    "plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "plt.xlim(left=-1, right=401)\n",
    "ticks = plt.xticks()[0]\n",
    "labels = [str(int(t)) if t not in [350, 400] else (\"CT\" if t == 350 else \"CPT\") for t in ticks]\n",
    "plt.xticks(ticks, labels, rotation=45)\n",
    "ticks = ticks[1:-1]\n",
    "labels = labels[1:-1]\n",
    "plt.grid(axis='y')\n",
    "plt.xticks(ticks, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5e0d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#592A37']\n",
    "width = 15  # bar width\n",
    "trials = df_eval[\"trial\"].astype(int)\n",
    "plt.bar(trials - width/2, df_eval[\"accuracy\"], width=width, label=\"CIM Accuracy\", color=colors[0])\n",
    "plt.bar(trials + width/2, df_eval[\"orig_accuracy\"], width=width, label=\"Accuracy\", color=colors[1])\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"Trial\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"CIM Accuracy vs. Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../../Paper/figs/intervall_acc.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e5fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3432dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = df_eval[\"orig_accuracy\"] - df_eval[\"accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfafb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359ccf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index as independent variable\n",
    "X = pd.Series(diff.index, name=\"x\")\n",
    "X = sm.add_constant(X)  # adds intercept term\n",
    "y = diff\n",
    "\n",
    "# Fit model\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b661eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.Series(df_eval[\"trial\"], name=\"trial\")\n",
    "X = sm.add_constant(X)  # adds intercept term\n",
    "y = df_eval[\"accuracy\"]\n",
    "\n",
    "# Fit model\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hwnas-nni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
